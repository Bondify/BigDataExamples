{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd29c03e250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /raw/simple-ml\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r /raw/simple-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -mkdir -p /raw/simple-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -put /home/cloudera/Hadoop/spark/data/simple-ml.json /raw/simple-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   1 cloudera supergroup       7559 2018-02-13 13:55 /raw/simple-ml/simple-ml.json\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /raw/simple-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLib in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/raw/simple-ml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "|green|good|    12|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "|green|good|     1|14.386294994851129|\n",
      "|green|good|    12|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "|green|good|    12|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"value2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised = RFormula().setFormula(\"lab ~ color + value1 + value2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedDF = supervised.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparedDF = fittedDF.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- lab: string (nullable = true)\n",
      " |-- value1: long (nullable = true)\n",
      " |-- value2: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|color| lab|value1|            value2|            features|label|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|green|good|     1|14.386294994851129|[0.0,1.0,1.0,14.3...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|[0.0,0.0,8.0,14.3...|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|[0.0,0.0,12.0,14....|  0.0|\n",
      "|green|good|    15| 38.97187133755819|[0.0,1.0,15.0,38....|  1.0|\n",
      "|green|good|    12|14.386294994851129|[0.0,1.0,12.0,14....|  1.0|\n",
      "|green| bad|    16|14.386294994851129|[0.0,1.0,16.0,14....|  0.0|\n",
      "|  red|good|    35|14.386294994851129|[1.0,0.0,35.0,14....|  1.0|\n",
      "|  red| bad|     1| 38.97187133755819|[1.0,0.0,1.0,38.9...|  0.0|\n",
      "|  red| bad|     2|14.386294994851129|[1.0,0.0,2.0,14.3...|  0.0|\n",
      "|  red| bad|    16|14.386294994851129|[1.0,0.0,16.0,14....|  0.0|\n",
      "|  red|good|    45| 38.97187133755819|[1.0,0.0,45.0,38....|  1.0|\n",
      "|green|good|     1|14.386294994851129|[0.0,1.0,1.0,14.3...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|[0.0,0.0,8.0,14.3...|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|[0.0,0.0,12.0,14....|  0.0|\n",
      "|green|good|    15| 38.97187133755819|[0.0,1.0,15.0,38....|  1.0|\n",
      "|green|good|    12|14.386294994851129|[0.0,1.0,12.0,14....|  1.0|\n",
      "|green| bad|    16|14.386294994851129|[0.0,1.0,16.0,14....|  0.0|\n",
      "|  red|good|    35|14.386294994851129|[1.0,0.0,35.0,14....|  1.0|\n",
      "|  red| bad|     1| 38.97187133755819|[1.0,0.0,1.0,38.9...|  0.0|\n",
      "|  red| bad|     2|14.386294994851129|[1.0,0.0,2.0,14.3...|  0.0|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = preparedDF.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression() \\\n",
    "    .setLabelCol(\"label\") \\\n",
    "    .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedLR = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fittedLR.transform(train).select(\"label\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()   \\\n",
    "    .setMetricName(\"areaUnderROC\") \\\n",
    "    .setRawPredictionCol(\"prediction\") \\\n",
    "    .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(fittedLR.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedLR.write().overwrite().save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - cloudera cloudera          0 2018-02-13 13:57 model/data\r\n",
      "drwxr-xr-x   - cloudera cloudera          0 2018-02-13 13:56 model/metadata\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "outputs": [],
   "source": [
    "rForm = RFormula()\n",
    "lr = LogisticRegression() \\\n",
    "    .setLabelCol(\"label\") \\\n",
    "    .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [rForm, lr]\n",
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParamGridBuilder() \\\n",
    "    .addGrid(rForm.formula, [\"lab ~ . + color:value1\", \"lab ~ . + color:value1 + color:value2\"]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])  \\\n",
    "    .addGrid(lr.regParam, [0.1, 2.0])  \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()   \\\n",
    "    .setMetricName(\"areaUnderROC\") \\\n",
    "    .setRawPredictionCol(\"prediction\") \\\n",
    "    .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs  = TrainValidationSplit() \\\n",
    "    .setTrainRatio(0.75) \\\n",
    "    .setEstimatorParamMaps(params) \\\n",
    "    .setEstimator(pipeline) \\\n",
    "    .setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsFitted = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9130434782608696"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(tvsFitted.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = tvsFitted.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "best.write().overwrite().save(\"bestmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelineModel.load(\"bestmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9130434782608696"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[color: string, lab: string, value1: bigint, value2: double, features: vector, label: double, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('green', 1, 14.386294994), ('blue', 1, 14.386294994)]\n",
    "\n",
    "df = spark.createDataFrame(l, ['color', 'value1', 'value2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fitted = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------+--------------------+--------------------+--------------------+----------+\n",
      "|color|value1|      value2|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+------+------------+--------------------+--------------------+--------------------+----------+\n",
      "|green|     1|14.386294994|[0.0,1.0,1.0,14.3...|[-0.3258182494576...|[0.41925845399245...|       1.0|\n",
      "| blue|     1|14.386294994|(7,[2,3,6],[1.0,1...|[1.37391263353745...|[0.79801156290428...|       0.0|\n",
      "+-----+------+------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fitted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
